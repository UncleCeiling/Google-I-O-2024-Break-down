# ðŸŽ¤ Google I/O 2024

## ðŸ«– Some pre-expo drama

Less than 24 hours before the Google I/O was set to kick off, **OpenAI announced their GPT-4o** model.

Looking at what OpenAI demo-ed, then watching Google I/O, it became clear the **OpenAI want to been see as the leaders** in GPT and AI tech. Whether that's true or not, I can't say, but we can definitely try to assess it as we work through everything Google showed off.

## ðŸ‘‹ Intro

Google's I/O event is split into 2 parts; a **main event**, and a **"Developer" event**.

This article/summary/thing is going to **ignore that distinction** and try to combine them.

The event started off with an Musical introduction from [Marc Rebillet](https://www.marcrebillet.com/), using one of Google's new **AI Music tools** to do a set based of random prompts and **samples generated by AI**. A interesting look at a few things that would rear their heads later.

## ðŸ… Achievements

Google where keen to **show off their achievements**, everything from **scientific** advances to **humanitarian** and **accessibility** work. They also highlighted their continued involvement in the UN's **Data Commons** and **Sustainable Development Goals** initiatives.

### ðŸ§ª Science

---

|Project|Description|
|:-:|:-|
| `RT-2` |Turning **Vision and Language into action** with robotics|
| `SIMA` |Automated navigation of virtual **3D environments**|
| `AlphaGeometry` |Solving **Olympiad maths problems** (like Wolfram Alpha, but with a PHD)|
| `GNoME` |**Materials Engineering and synthesis** research|
| `AlphaFold` |Modelling complex **protein folding** and other **molecular behavior**. Used by **1.8 million scientists** across **190 countries**.|
|`Flood Forecasting`| In **80+ countries** |

---

### ðŸ¦¾ Accessibility

---

- Working with EnAble and Incluzza (Building **accessibility into Android**)
- Using **on-device AI** to expand text-readers' capabilities to include image description.
- Navarasa **Indic language translation** project (**15 languages** so far)

## ðŸ–¥ï¸ Hardware

While the focus of the event was AI, Hardware also featured throughout, though mostly with reference to some **key industry announcements**:

### ðŸ§  Trillium TPU

---

TPUs (**Tensor Processing Units**) are vital for Google's shift towards **AI infrastructure and services**. The Trillium TPUs boast "**4.7x the performance**" of the previous generation (whatever that means). These TPUs will be made available for use **via their cloud services**.

### ðŸ’ª Axiom ARM CPU

---

Google's first real foray into **ARM CPUs**; the focus of the Axiom has been to produce a **performant and energy efficient CPU**, potentially pointing toward **mobile computing** (Laptops and tablets) as its intended market.

### ðŸŽï¸ NVidia Blackwell

---

Google have been **partnering with NVidia** a lot recently, and this has made top-end NVidia hardware more and more **accessible via Google's Cloud infrastructure**. Blackwell will be **available via Google Cloud by early 2025**.

### ðŸ§° AI Hypercomputer

---

"**Hypercomputer**" is Google's term for a new type of supercomputer that is evolving out of the need for AI infrastructure.

Characterised by a **focus on AI workloads** and the **presence of liquid-cooling**; Hypercomputers are Google's answer to the issues surrounding AI computing requirements.

Google have actually **implemented liquid cooling on 1 GigaWatt of their fleet** so far.

They have also made sweeping improvements to their **networking infrastructure**; they now have **2 million miles of fiberoptic** under-sea and over-land cables.

## ðŸ¤– AI

Google's **Deepmind** had a lot to show off this year and Google as a whole has clearly put AI at the top of their priorities.

### â™Š Gemini

---

Gemini is Google's **Flagship** AI model.

With a focus on **multi-modality**, Google intend to make AI **applicable** sooner rather than later. Multi-modality is an interesting domain of AI research that holds a lot of interesting prospects. Being able to take **images, text *and* audio** as an input allows the model greater context, allowing for more precise results, more appropriate responses and better application.

To support this, Google has put a lot of effort into building out their context-window (**the amount of tokens the model can ingest per prompt**) to a whopping **1 million tokens**. Comparing that to OpenAI's **GPT-4 with only 32,768 tokens**, it's clear that this is a big leap from what we're used to. For context:

|Media|1 million tokens|
|-|-|
| `PDF` | **1,500** pages |
| `Code` | **30,000** lines |
| `Video` | **1** hour |

And to hammer this all home, Google announced that for Pro users they've upped this context window to a staggering (for now) **2 million tokens**, with a stated **goal of reaching unlimited tokens** to allow for the best possible experience.

Google were also keen to show off how Gemini can be **linked to their panopticon of services** to perform some pretty impressive feats:

> "What is my license plate number?"

A seemingly simple question, but surprisingly complex on inspection. Gemini started by **filtering emails and texts** to try and find the information there. When it couldn't find the information there, it pulled out all the **images of cars in the user's Google Photos**, tried to deduce which car was the user's car, then **read the license plate**. The response was **simple and without the usual waffle** we've come to expect from GPTs;

`Your license plate number is ...`
followed by an **image of the car** to back up its assertion.

The next Demo came in 2 parts:

> When did my daughter learn to swim?

Once again, Gemini scampers off into the recesses of the user's Google data, checking emails, calendar events, messages and photos. Without much delay it comes back with an answer, **based off an email containing dates for swimming lessons, images of a certificate in the user's photos and by identifying a picture of the user's daughter**. It presents it's answer simply and backs it up with images. Now comes the impressive part:

> Show me how she's progressed

A subtle prompt that **takes all that previous context** and explodes it out. Gemini now has a lot to do in one go:

1. **Correctly interpret** "she" and "progress" from the previous context.
2. Work out that it needs to **build a narrative** around this context.
3. Find **appropriate images** to effectively convey this narrative.

This one takes a little longer. If I were the speculative type I might point out here that **Google have a history working with specialised models** and are known for building custom tools for specific jobs; they have some **tools that can already create narratives** from Google Photos using meta-data and simpler AI tools. My guess is that Gemini has been built to **leverage these tools** when it can.

Regardless of how it's done, within the time it's taken you to read that last paragraph, Gemini has produced a short affirmative response and now **presents a collage** highlighting the user's daughter in a pool as a baby, learning to swim as a child, an image of their certificate and them, and of them now considerably older, but still enjoying swimming.

|||
|:-:|:-:|
| Is it **impressive**? | `Yes` |
| Does it have **vision**? | `Yes` |
| Does it raise some **serious questions**? | `Also Yes` |

Will Gemini become the Google Assistant we always wanted? Or just another avenue for people and their data to be exploited?

> I guess we'll just have to wait and see...

In the meantime! Gemini Pro is available for **Google Workspaces**, doing things like:

- **Managing and summarising** emails (or entire email exchanges) and chats.
- **Summarising, transcribing and translating** recordings of meetings as well as some live meetings.
- Helping to **draft** correspondences.
- Performing **QnA** about emails, documents, etc.
- Basic **RPA** and **meta-tasks** like pulling latent information into a spreadsheet.

Gemini was also shown performing some **basic Data-Analysis**, taking **data from spreadsheets** and **writing code to make useful charts** from them. Even better, it would **provide the Python script** (using **Pandas** and **MatPlotLib**)

**Trip-planning** is also coming to **Gemini Advanced** users later this summer. The feature attempts to leverage the multi-modal aspects of Gemini for a **really cool use-case**. This one is **niche**, so check out the [Demo](https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Gemini_planning_KHxnn35.mp4) for a better glimpse at how it works.

#### ðŸ§‘â€ðŸ’¼ Gemini AI Team-mate

---

Gemini Team-mates are **Gemini agents** with their **own account** in the Workspace.

They can be given a **Job Description**, **Job Role** and other **Instructions** as prompts, then it will use **chats, emails and files** to build responses to messages and even **take action**.

The main advantage of this model is that it requires **very little set-up or development** compared to similar tools that one might build as part of RPA or building a chat-agent.

A little **dystopian**! But **interesting** nonetheless...

#### âš¡ Gemini Flash

---

Google announced Gemini Flash, their new **lightweight** version of Gemini. Flash is supposed to be **faster, cheaper, and optimised for low latency**.

While Flash compromises in the size and complexity of the model, for **1/10th the price** per million tokens it's a great opportunity for **new markets** to use the technology without breaking the bank (and in **theory** with a **lower ecological impact**), while still using the **1 million token context limit**, Flash is set to be *the* commercial **model of choice for low-latency, low-context** users.

#### ðŸ’Ž Gems

---

Gems are Google's answer to crafting **"Expert" or "Specialist"** Gemini agents. Rolling out soon for **Gemini Advanced** users, later to **Workspaces** and **Pro** users.

By **providing a description** of what you want your Gem to do and how, you can create agents **for any reasonable purpose**. Google gave some examples of the kinds of things they expect people to use this feature for;

- Personal trainer
- Sous chef
- Pair-Coding partner
- Bedtime Story-teller
- Creative-writing Editor

Interestingly, a Gem is used to help **expand the original instructions** you provide. The idea is that this "Prompt" Gem (for lack of better name) can **help you write better AI prompts** resulting in more effective Gems.

### ðŸ™‹â€â™€ï¸ Gemma

---

Gemma is Google's **Open version of Gemini**, with a variety of different versions available. Google added 2 new versions to the Gemma family of models.

#### ðŸ“· PaliGemma

---

Google's **first Open vision-language** model. It's built for **image captioning**, **visual Q&A** and **image labeling**. PaliGemma is **pre-trained for these tasks** and joins the 2 other pre-trained Gemma versions;

- RecurrentGemma (A text-generation model built for memory efficiency and throughput)
- CodeGemma (A collection of models for doing coding tasks)

#### ðŸ‘­ Gemma 2

---

Gemma 2 is Gemma's **next-generation** and is **currently being built** from the lessons of Gemini, with the largest parameter version jumping **from 7 billion parameters to 27 billion**.

Optimisation for **TPUs and GPUs** was also high on Google's list of priorities for Gemma 2.

#### ðŸ¯ Navarasa

---

Google also showed off a version of Gemma **fine-tuned for translating Indic languages**. This was built as part of a project to allow all the Indic languages to be **preserved**, **understood** and to allow the speakers to **communicate freely** with each other. So far, they have **15 of these highly localised and unique languages** working well with the model.

### âš›ï¸ Gemini Nano

---

This is Google's **On-device AI model**. It is currently **being built into Android and Chrome Desktop**.

The idea is that Nano allows apps and systems to use **AI functionality without having to send data off-device**. I'll talk about this some more during the **security** section, but this has a lot of **potential**.

### ðŸŒŸ Project Astra

---

Project Astra is **Google's vision for what AI can do for us**. Put short, it takes Gemini and cranks it up to 11:

- **Multi-modal** input
- **Low-latency**
- **Conversational** tone

It's everything Google dreamed that it's **"Hey Google" assistant could have been**. They focused on trying to make **interactions as natural as possible**, using **context-caching**, **intonation** and **rapid responses** to make the best of the Gemini model.

So here's where I link back to the Tea/Drama at the beginning about OpenAI's announcement. In my opinion Project Astra is what they were **trying to undermine**.

I encourage you to **check out these 2 announcements yourself** and draw your own conclusions, but the parallels are stark.

| 13th May| 14th May |
|:-:|:-:|
| [OpenAI GPT-4o](https://www.youtube.com/watch?v=DQacCB9tDaw)| [Google Project Astra](https://www.youtube.com/watch?v=nXVvvRhiGjI)|

Either way, I'll be **keeping eye on Project Astra**; they may **resurrect the Google Glass**, and with them my hopes of a truly trans-human future!

### âŒ¨ï¸ AI Dev

---

Google have added a few **key features** to their APIs:

|Feature|Description|
|:-:|:-|
|[Video frame extraction](https://ai.google.dev/gemini-api/docs/prompting_with_media?lang=python#prompting-with-videos)|Gemini can now accept **videos and frames** as part of prompts.|
|[Parallel function calling](https://ai.google.dev/gemini-api/docs/function-calling/tutorial?lang=python#parallel_function_calling)|Instead of having to call multiple functions one after another, Gemini can now **send and receive multiple function calls and responses in one message**.|
|[Context caching](https://ai.google.dev/gemini-api/docs/caching)|Given the increasing size of context windows, the ability to **cache parts of the prompt** to act as context for future calls greatly **reduces the amount of redundant data sent** as part of a request and also **reduces latency and workload** for the server.|

They also announced a **competition** for "**Best Overall AI-enabled App**", with the winner receiving a **custom electric DeLorean**. They had a cute announcement video **starring Doc (Christopher Lloyd)**

It was also announced that Gemini is **available in all of these platforms**:

- Android Studio
- Chrome Devtools
- Project IDX
- CoLab
- VSCode
- IntelliJ
- Firebase

### ðŸŽ“ LearnLM

---

This is Google's attempt to build a **Gemini model for education** and is essentially a fancy Gem, and in fact will be **made available alongside other pre-made and refined Gems**.

Built in collaboration with **MIT**, **Teacher's College**, **Arizona State University** and **Khan Academy**; LearnLM is intended to **build understanding** instead of just providing answers.

It's designed to be useful for developing useful **pneumonics**, **analogies** and **exploring topics naturally**. YouTube integration is designed to allow students to **ask questions about videos** they are watching, and can even be used to **quiz students on what they are seeing**, exploring any concepts that aren't being absorbed.

With the help of **MIT Raise**, Google put together a free "**Generative AI for Educators**" course available [here](https://grow.google/ai-for-educators/) for anyone wanting to try it.

On top of this, Google is also beginning to **integrate Gemini into Google Classroom** to support Instructors.

## ðŸ§± Generative AI

Google have a **variety of Generative AI projects**. As you'll see below they're pretty **varied in scope and execution**, but they are all used by Google to **advance their understanding** of the field. The lessons learned here are **directly applied to their flagship models** and seem to be informing their **goals in the AI space**. Might be worth keeping that in mind as you read.

### ðŸ—’ï¸ Notebook LM

---

We all know **Google is a fan of their toys** and Notebook LM is a great example.

Built as a way to **integrate LLMs into learning environments**, Notebook LM allows you to **upload materials** on a given topic or class, then **ask directed questions** about the materials to an AI. You can have it **create a Learning Guide**, **Quizzes** or even help you **Revise certain topics**.

The new feature that was shown off, was the ability to **create a "Podcast" of sorts, called an "Audio-overview"**. Simply put, Notebook LM takes the material and **generates a Podcast-styled conversation** between 2 presenters. Notably one presents as feminine and one as masculine, with both being **cheery, youthful and with the energy you'd expect** from an educational podcast.

The coolest part is that this Podcast is interactive! Much like radio-hosts taking callers, **you can ask questions (verbally or by typing)** and the presenters respond. During the "Audio-overview" demo, they interacted and asked the system to **explain the concept in a way that their 3rd grade child might understand**, adding that the child was really into basketball. **Without any perceivable delay**, the "Presenters" picked up the question like a radio presenter might, simplifying the topic and applying appropriate analogies.

Cool stuff!

### ðŸ“º Veo

---

Already available through **VideoFX in Google Labs**, Veo is Google's best text-to-video model to date. Generating **1080p** videos **just beyond a minute** in length.

[Click here](https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Gen_Media_Blog_Veo_Supercut.mp4) to view a super-cut of some **unmodified Veo outputs**.

Google seems really keen to highlight that **Veo is a tool** for creators to use, supplementing the rest of their tools, but not intended to replace any. To hammer this home they **partnered with Donald Glover** and his studio **Gilga**.

[Donald Glover and Gigla experimenting with Veo](https://youtu.be/dKAVFLB75xs)

One of the key technical features of Veo is that it uses a **combination of architectures** from Google's other generation projects to improve performance and efficacy.

### ðŸŽ¼ Music AI Sandbox

---

Aside from being a fun opener for the event, the Music AI Sandbox got it's own spotlight.

Google wanted to highlight their focus on working **with** artists, not **instead** of artists. Having Marc Rebillet make music with AI generated samples was their way of trying to demonstrate this. Marc even talked about how using the Sandbox encouraged him to try sounds, and combinations of sounds, he **wouldn't have tried otherwise**.

Wyclef Jean was show using the tool and compared it to **digging through vinyl crates** looking for sample-able material. While it is definitely **better to empower artists** with audio AI tools than using that AI to take power away from artists, there are still fundamental questions that need answering;

- How can we **discern** AI-music from human made music?
- Can we find ways to **protect artists from exploitation**?
- What **data is being used** to train these Generators?
- Do these datasets **violate artists' copy-rights**?

These are **non-trivial questions** and will require a society-wide conversation around **what we consider creation**, **how we value art** and music in our society, and ways that we can **encourage creation and exploration without enabling exploitation**.

[Click here](https://youtu.be/-dPqc7l2zu8) to watch and listen to what Google's **partnered artists** think of the tool.

We'll see where this one goes and **what the world thinks** of it.

### ðŸ–¼ï¸ Imagen 3

---

Imagen got an update! Imagen 3 is Google's newest text-to-image tool. It has a **focus on accuracy**, remembering **small details**, and is much better at **suppressing artifacts**. An image is worth a thousand words (or tokens?) so **take a look for yourself**.

| Image | Prompt |
|:-:|:-:|
|![AI generated image of a wolf](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Copy_of_Copy_of_WM_A_close_up.max-1024x1024.format-webp.webp)| A close up of a sleek wolf perched regally in front of gray background, in a high-resolution photograph with detailed fine details, isolated on a plain stock photo with color grading in the style of a hyper-realistic style.|
|![AI generated image of a man](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Copy_of_Copy_of_WM_A_photo_of.max-2000x2000.format-webp.webp)| A photo of a man with short hair and beard smiling at the camera. The background is blurry and it shows trees and buildings in light colors.|
|![AI generated image of a squirrel in a boot](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Copy_of_Copy_of_WM_squirrel_b.max-2000x2000.format-webp.webp)| A pair of well-worn hiking boots, caked in mud and resting on a rocky trail. The head of a squirrel is poking out of one of the boots, and it looks lazily at the camera, a little king of its shoe. The laces of both boots fall loosely to the ground. There's a mountainous landscape in the background. Cinematic movie still, high quality DSLR photo. |
|![AI generated image of 3 women laughing in front of a sunset](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Copy_of_Copy_of_WM_Three_wome.max-2000x2000.format-webp.webp)| Three women stand together laughing, with one woman slightly out of focus in the foreground. The sun is setting behind the women, creating a lens flare and a warm glow that highlights their hair and creates a bokeh effect in the background. The photography style is candid and captures a genuine moment of connection and happiness between friends. The warm light of golden hour lends a nostalgic and intimate feel to the image. |
|![AI generated image of a sculptor](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Copy_of_Copy_of_WM_Clay_sculp.max-2000x2000.format-webp.webp)| A view of a person's hand as they hold a little clay figurine of a bird in their hand and sculpt it with a modeling tool in their other hand. You can see the sculptor's scarf. Their hands are covered in clay dust. a macro DSLR image highlighting the texture and craftsmanship. |
|![AI generated image of a wooden robot in field of flowers. Anime-styled.](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Copy_of_Copy_of_WM_wooden_rob.max-2000x2000.format-webp.webp)| A weathered, wooden mech robot covered in flowering vines stands peacefully in a field of tall wildflowers, with a small bluebird resting on its outstretched hand. Digital cartoon, with warm colors and soft lines. A large cliff with waterfall looms behind. |

Google also took time to highlight that Imagen 3 is much better at **rendering text** than it's predecessors:

|![AI generated image of the entrance to a central library](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Copy_of_Copy_of_WM_A_photogra.max-2000x2000.format-webp.webp)|![AI generated pixel art of the STS-1 shuttle launching](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Copy_of_Copy_of_WM_STS1.max-2000x2000.format-webp.webp)|![AI generated image of rainbow feather making the word "LIGHT"](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Copy_of_Copy_of_WM_Word_light.max-2000x2000.format-webp.webp)|
|:-:|:-:|:-:|
|A photograph of a stately library entrance with the words "Central Library" carved into the stone.|Pixel art of a space shuttle blasting of. Cape Canaveral in the background, blue skies, with plumes of smoke billowing out. "STS-1" is written below it.|Word â€œlightâ€ made from various colorful feathers, black background.|

All this advancement in **fidelity has come at cost** though. Google have had to **ramp up their evaluation efforts** and now use a **dedicated Red-Teaming** process to try and mitigate the associated risks of using Gen-AI to make images. I'll touch on this some more in the security section.

Imagen 3 is available through **ImageFX in Google Labs**.

## ðŸ”Ž Search

Already **rolled out in the US** on the 15th May, Google's new Search architecture was a hefty part of the main event. You might have seen some of the **fallout** already, but I'll try to give you some context:

### ðŸ“œ Context

---

Google's new Search combines **Gemini**, **real-time data** and Google's **ranking system** to try and better facilitate searchers.

However, **Google's ranking system has been rapidly degrading**; at first, due to **SEO optimisation** and increasing **bot presence**, but more recently the introduction of Gen-AI has resulted in making **Google's ranking algorithms less and less useful**.

This led to Google making a serious of **sweeping** changes, all of them **unpopular**, that resulted in nothing but disgruntled users and reduced figures.

### ðŸ– The Meat of it

---

Google's answer? `AI`

Predictable given the current tech climate, but Google *seemed* to have thought this one through; The basic premise was that **Gemini would Google things *for* you**.

In practice this means:

#### 1. Prompt

> You write your **question or questions** into the search box in natural language.

OR

> Use your **camera and microphone** to ask the question in context.

#### 2. Break-down

> Gemini breaks your request down into **smaller requests** and processes them individually, **preserving context** across all of them.

#### 3. Query & Collate

> Gemini filters and **processes the results** of the queries **within the context of the original question**.

#### 4. Process and Present

> Gemini further processes the results to provide **context-sensitive options and groupings**.

The [Demo](https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/AI_Overviews_-_Complex_Questions.mp4) for this is very convincing, but the real response has been mixed.

Hallucinations are often to blame for bad AI responses, but **Gemini's issue *seems* different**. User's are complaining that Gemini is lying to it or hallucinating answers, but the issue seems to be more to do with **Gemini blindly trusting sources** that it shouldn't be.

Take this example of a user who was looking for **why their cheese wasn't sticking to their pizza**:

![Gemini suggesting that the user add glue to the sauce](https://www.hindustantimes.com/ht-img/img/2024/05/24/1600x900/google-ai-cheese-pizza-glue-viral-post_1716534648801_1716534658293.jpg)

Gemini does it's best at gathering a **variety of answers**, but it makes the mistake of using Reddit as one of it's sources, unaware that **Reddit users are not a very serious bunch**. Interestingly, in advertising and film-production, **glue is often added to foods to make them more appetising** (including the cheese on pizza).

This highlights one of the **inherent issues** with using AI like this:

- AI doesn't understand **sarcasm and humour** that well.
- AI often misses the **nuances involved online conversations**.
- AI can often **mis-attribute valid information** for one situation as being valid for another.

To Gemini, this was a perfectly **valid answer that was backed up with a source** online, by all means the kind of thing worth telling the user. Which leads me to question Google's **catchphrase** for the idea:

`Let Google do the Googling for you`

A nice idea, sure. But the **reality** of trusting AI to do your work for you is that;

**It *isn't* you, and it *will not* do things the same way that you would.**

This is something we all seem to understand about **working with humans**;

> "If you want something done right, do it yourself."

I wonder when it will sink in that **we need to consider the same factors** when working with AI.

## ðŸ“± Android

With all the AI mentioned above, you can guarantee **Google are adding more AI capabilities to Android**:

In fact, they're **transitioning the Google Assistant to Gemini** on Android. "**Circle to search**" is also being **expanded with Gemini**'s capabilities to be able to **more complex Maths** on-device.

Android now includes a **Foundation model built into the OS** (Gemini Nano). This model can be used for **fast, on-device processing**. This should allow better **security use-cases** and enable AI features even if there's **no network connection**.

Google showed how they're already applying these on-device models to provide the **Talk-back accessibility features** with the ability to **label unlabelled images without using data**.

I'll talk about the **security implications** a bit more in the security section.

## ðŸ“² Android Dev

I'm **not an Android dev** so I'll just hit you with the **headlines**:

### - Gemini API, Vertex API, Gemini Nano and MediaPipe LLM are all available through Android Studio

### - Android Studio now fully supports Kotlin Multiplatform libraries

(Google Docs has also been migrated to Kotlin MP on Web, Mobile and Desktop)

### - Google now encourage app-makers to use the Adaptive dev form for their apps

(Build once, use everywhere)

### - Handwriting support is improved and expanded

### - Widget support is also expanded

### - Device emulation and checking has been expanded

### - Gemini can now be used directly in Android Studio

(This can be used to better transform, optimise and document code)

### - Gemini can use text and images to build apps from wireframes and descriptions

Hope I didn't miss anything there!

## ðŸ•¸ï¸ Web

Google have remained committed to **expanding WebAssembly**, including adding **Web GPU support** to better make use of on-device hardware for AI.

Google also revealed **SpeculationRules API** and **ViewTransitions API**:

SpeculationRules allows developers to **pre-fetch and pre-render pages**, reducing **latency**, improving **responsiveness** and **spreading the workload for loading pages** over a longer time.

ViewTransitions allows users to move from one page to another **without losing their page contexts**. It can even be used for **multi-page apps and sites**.

I'm going to add these **announcements** here too, while not *strictly* Web-dev they're appropriate;

- **Flutter got WebAssembly support** for web apps.
- **Firebase** can not connect directly to **Google Cloud SQL** via Data Connect.
- **Firebase** supports Cloud **Vector-search** and Cloud **Function calling**

## âŒ¨ï¸ Project IDX

Project IDX is Google's answer to "How do we make an **integrated IDE for cloud platforms**?". IDX is **open to everyone**, can import **existing repos** and contains a variety of **templates**. If VSCode is an old friend for you, then using IDX will be like meeting that friend after they were abducted by aliens; **essentially the same**, but there are some big differences.

The first major difference is that IDX will host your **environment in a VM in the cloud**, so even though you're using a **web-IDE**, you can still run and test your software like you would normally. Honestly it's a little eery at times, especially when you remember that these **VMs are hosted in Google's Cloud free of charge**.

> "If it's free, you're the product"

But hey, I won't look a gift horse in the mouth! If you're interested, **give it a go**, but just keep in mind that you'll have to **manually import your VSCode preferences**; and while **IDX uses VSX for it's extensions** (the VSCode extension library), **some extensions aren't fully supported** yet.

Before I forget! Google also added the **following integrations** to IDX to make it more useful for **users of their ecosystem**:

- Google Maps
- ChromeDevTools
- Lighthouse
- Cloudrun

## ðŸ›¡ï¸ Security

**Congratulations** on making it to the final part of this deep-dive!

Here I'm mostly going to ramble about the **security implications** and aspects associated with the stuff we've already talked about; but before that I'd like to highlight **some of the things Google talked about** with regards to their **security concerns and developments**.

The event featured an "Ethics" section which mostly served to try and highlight that the dystopian mega-corp we all know and love *is* **doing some good in the world**. I covered most of that stuff at the beginning, but they also included some really interesting insights into their **security ethos and external priorities**.

### ðŸ‘¿ AI is dangerous?

---

The first of these priorities is that Google are pushing the use of **Red-Teaming and pro-active mitigation** measures as a way to help reduce the risk of misuse. They talked about making use of **internal and external expertise** to try and shore up their systems against all types of mis-use (their most emphasised concern), but the **details of how** exactly they went about this were **notably absent**.

Combatting **misinformation** popped up here and there throughout the event. Google were keen to announce the introduction of **SynthID for watermarking Images, Audio, Text and Video**. They didn't talk about the exact techniques that SynthID uses, but that *is* **to be expected** for this sort of technology, and **we'll know soon** enough because...

In **promising news**, Google did say they were **Open-sourcing SynthID for Text** as part of a "**Responsible Generative AI Toolkit**". Google have been working closely with the C2PA (the **Coalition for Content Provenance and Authenticity**) to develop more standards and tools for wider use.

They were also keen to talk about their **ongoing collaborations** with companies like:

| Adobe | BBC | Intel | Microsoft | OpenAI | Publicis Groupe | Sony | And more...|
|-|-|-|-|-|-|-|-|
|||||||||

### ðŸ‘¼ AI is anti-danger?

---

There was also a focus on highlighting **how AI can be used to help** protect people, and how different architectures and models can used in conjunction to **improve reliability and help protect** people.

One such example was the use of **On-device models**. Google are convinced that putting **simpler, more efficient models on device** means we can use those models to engage in more **advanced and nuanced security activity without having to share data**. To highlight this point, they **demonstrated a scam-call**, showing how an **on-device model can process the call in real-time**, **without sending the audio** off to the cloud for analysis, and effectively **prevent Vishing scams** for many users.

While **many still remain sceptical** about this idea and whether **Google will *really* keep your data on-device or not**, Google seem pretty adamant that they will and that it'll work. Personally, I think this is the **best compromise I've seen** coming from a corporation this large. While companies like **Apple are getting bogged down in legal battles** for their business interests, it *seems* like Google is **at least trying to work *with* legislators** and advisory bodies. **Only time will tell**!

On a more **hypothetical note**; it was mentioned a couple times during the event that on-device models could be used for **anonymisation or abstraction**, with the model on the device "**pre-processing**" your personal data **before it is sent off-device**. A promising idea and an understandable one given how Google has tried to approach the **issues with cookies and advertising**;

> **Side note:**
>
> Google has been trying to develop replacements for 3rd party cookies for a while now. Their latest answer was "Topics", essentially your browser uses your data to make a list of things you might be interested in, then when a site wants to advertise to you, instead of the site receiving *all* of that data, it would receive a very limited number of randomised topics from the list. It's smart, but has been criticised for the fact that it would essentially make Google the new Facebook.

This "**anonymise, then send**" technique is promising and is the premise for a **variety of security techniques**. Most users understand that **businesses need *some* information about us** to be useful to us; the problem arises when businesses have enough information to **track us outside of our interactions** with them. Here's hoping that these sorts of **anonymisation and blurring** can help with that. If not, maybe it's time for a larger conversation and some sort of "Data Treaty", a truce of sorts.

As a **final note** before I wrap up; Google announced "**Google Checks**", an **AI powered compliance platform**, aimed at tying together:

- What the product is required to do.
- What the product actually does.
- What you say the product is doing.

It looks promising, but I can only imagine the  headache that the Compliance team for "Checks" will have by release!

## ðŸŽ‰ Thanks for reading

So that's it! That's my break-down of the Google I/O 2024 event!

"AI" was mentioned over a hundred times during the event and only 38 times in this article (yes, including that one) so hopefully I captured the essence of it.

If I made any mistakes or people have any questions/comment, do feel free to reach out!

> **No AI was used in the making of this break-down.**
>
> *(Which is why it took 3 weeks and I burnt out twice...)*

## ðŸ”— Links

- [100 things we announced at I/O 2024](https://blog.google/technology/ai/google-io-2024-100-announcements/)
- [Google blogs about I/O 2024](https://blog.google/technology/developers/google-io-2024-collection/)
- [Google Keynote (Google I/O â€˜24)](https://www.youtube.com/watch?v=XEzRZ35urlk)
- [Developer Keynote (Google I/O '24)](https://www.youtube.com/watch?v=ddcZnW1HKUY)
